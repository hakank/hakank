/*

  Probability distributions in WebPPL.

  Here are several probability distributions in WebPPL, including
  variants of the built-in distributions.

  They are taken from the files 
    *_dist.wppl 
  at http://hakank.org/wppl/

  Note: 
  - all distribtutions are called <distribution>_dist
  - most of the implementations are from the excellent
    Handbook of probability distributions.

  Usage:
  $ webppl --require hakank_utils model.wppl
  
  For some distributions, there are also a closed form of quantiles.

*/


/*

  Bernoulli distribution
  Binomial distribution

  From Handbook on probability distributions, page 8
  """
  It is easy to simulate Bernoulli distribution with the following heuristic:
  * generate U from a uniform distribution,
  * compute X as 1 if U <= p and 0 otherwise.

  The binomial distribution is obtained by summing n i.i.d. Bernoulli random variates.
  """
 
  Example:
  var model = function() {

    var p = 0.8;
    var n = 10;
    var bern = bernoulli_dist(p);
    var bern2 = bernoulli(p);
    
    var binom = binomial_dist(p,n);
    var binom2 = binomial(p,n);    

    return {
        bern:bern,
        bern2:bern2,
        binom:binom,
        binom2:binom2,
    }
  }

  http://www.hakank.org/webppl/binomial_dist.wppl

*/
var bernoulli_dist = function(p) {
    var u = uniform(0,1);
    return u <= p;
}

// Quantile[BernoulliDistribution[p], k]
var bernoulli_dist_quantile = function(p,q) {
    return q > 1-p ? 1 : 0
}

/*
  Bernoulli
  https://en.wikipedia.org/wiki/Bernoulli_distribution
*/
var bernoulli_pdf = function(p,k) {
    return k == 1 ? p : 1-p
}

// Mathematica
var bernoulli_cdf = function(p,k) {
    return k < 0 ? 0 : (k >= 0 && k < 1 ? 1-p : 1)

}

/*
  Binomial
  https://en.wikipedia.org/wiki/Binomial_distribution
*/
var binomial_dist = function(p,n) {
    return sum(mapN(function(i) { return bernoulli_dist(p) }, n));
}

var binomial_pdf = function(p,n,k) {
    binomialf(n,k)*Math.pow(p,k)*Math.pow(1-p,n-k)
}

/*
  binomial_cdf(p,n,k) 
  https://www.radfordmathematics.com/probabilities-and-statistics/binomial-distribution/binomial-cumulative-distribution-function.html
*/
var binomial_cdf = function(p,n,k) {
    return sum(mapN(function(r) {
        binomial_pdf(p,n,r)
    },k+1))
    // Alternatives
    // return sum(mapN(function(r) { Math.exp(Infer(function() { binomial(p,n)==r }).score(true))},k+1))
    // return sum(mapN(function(r) { Math.exp(Binomial({p:p,n:n}).score(r)) },k+1))
}

var binomial_dist_quantile = function(p,n,q) {
    // return hakank_utils.binomial_quantileJS(p,n,q) // This is just fairly OK
    // Reversing the CDF.
    var t = mapN(function(i) {
        var x = binomial_cdf(p,n,i);
        return [i,x >= q]
    },n+1);
    var tt = filter(function(x) { x[1] == true  },t)
    return first(tt)[0]

}


/*
  Cauchy distribution

  Note: Even if the generation algorithm seems to be exactly the same as
  the built-in Cauchy there are quite different results. However, we see 
  the same effect of wildly different results if we compare with to
  runs of the built-in Cauchy. 
  See http://www.hakank.org/webppl/cauchy_dist.wppl for some experiments.

  From Handbook on probability distributions, page 86ff.
  """
  Since the quantile function is F^(-1)(u) = delta+gamma*tan((u-1/2)*pi), we can
  use the inversion function method.
  """

  Compared with the built-in cachy function:
  - delta: location
  - gamma: scale

  Example:

  var model = function() {
    var delta = 1;
    var gamma = 2;
    var g = cauchy_dist(delta,gamma);
    var g2 = cauchy({location:delta,scale:gamma}); // Built-in
    var g3 = cauchy({location:delta,scale:gamma}); // Checking another run
    return {
        g:g,
        g2:g2,
        g3:g3,
    }
  }

  http://www.hakank.org/webppl/cauchy_dist.wppl

*/
var cauchy_dist = function(delta,gammav) {
    var u = uniform(0,1); 
    return delta+gammav*Math.tan((u-1/2)*Math.PI);
}

var cauchy_dist_quantile = function(delta,gammav,q) {
    return delta+gammav*Math.tan((q-1/2)*Math.PI);    
}

/*
  Cauchy
  https://en.wikipedia.org/wiki/Cauchy_distribution
*/
var cauchy_pdf = function(delta,gamma,x) {
    return 1/(Math.PI*gamma*(1+Math.pow((x-delta)/gamma,2)))
}

// Mathematica: CDF[CauchyDistribution[delta,gamma],x]
var cauchy_cdf = function(delta,gamma,x) {
    return 1/2 + Math.atan((-delta + x)/gamma)/Math.PI
}


/*
  Chi distribution

  From Handbook on probability distributions, page 78
  """
  Take the square root of a chi-squared random variable.
  """

  Example:
  var model = function() {
    var k = 2
    var g = chi_dist(k);

    return {
        g:g,

    }
  }

  See http://www.hakank.org/webppl/chi_dist.wppl

*/

var chi_dist = function(k) {
    var x = Math.sqrt(sum(mapN(function(i) { return Math.pow(gaussian(0,1),2)},k)));
    return x;
}


/*

  Chi-squared distribution

  From Handbook on probability distributions, page 76
  """
  For an integer k, just sum the square of k normal variable.
  Otherwise use the algorithm for the gamma distribution.
  """
  
  Example:
  var model = function() {
    var k = 4
    var g = chi_squared_dist(k);
    return {
        g:g,
    }
  }

  See http://www.hakank.org/webppl/chi_squared_dist.wppl

*/
var chi_squared_dist = function(k) {
    var x = sum(mapN(function(i) { return Math.pow(gaussian(0,1),2)},k));
    return x;
}


/*

  Inverse Chi-squared distribution

  From Handbook on probability distributions, page 82
  """
  Simply inverse a chi-squared random variable
  """

  Example:
  var model = function() {
    var k = 4
    var g = inverse_chi_squared_dist(k);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/chi_squared_inverse_dist.wppl
  
*/
var inverse_chi_squared_dist = function(k) {
    var x = 1/sum(mapN(function(i) { return Math.pow(gaussian(0,1),2)},k));
    return x;
}


/*
  Non central Chi-squared distribution

  Note: Experimental
  I'm not sure how to interpret lambda: Is it the sum of the squares of the mu's
  or the actual parameter to the function?
  Also, how do we interpret the case with just k as a parameter, i.e. without lambda?


  From Handbook on probability distributions, page 79
  """
  For integer k degrees of freedom, we can use the definition
  of the sum, i.e. sum k i[n]dependent normal random variables
  N(sqrt(lambda,k),1).
  """
  
  http://www.hakank.org/webppl/chi_squared_non_central_dist.wppl

  Example

     var lambda_func = function(mu) {
       sum(mapN(function(i) {return Math.pow(mu[i],2)},mu.length));    
     }

     var model = function() {
        var k = 5;
        var mu = [1/2,1/3,1/4,1/5,1/6];    
        var lambda = lambda_func(mu);
        var g = non_central_chi_squared_dist(k,lambda);
        var g2 = non_central_chi_squared_dist_mu(k,mu);

        return {
           g:g,
           g2:g2,
           lambda:lambda
        }
    }

*/

var non_central_chi_squared_dist_mu = function(k,mu) {
    var x = sum(mapN(function(i) { return gaussian(mu[i],1)},k));
    return x;
}

// Here we use the lambda direct:
var non_central_chi_squared_dist = function(k,lambda) {
    var x = sum(mapN(function(i) { return gaussian(Math.sqrt(lambda/k),1)},k));
    return x;
}


/*
  Generalized Erlang distribution distribution

  From Handbook on probability distributions, page 64
  """
  The algorithm is very easy simulate independently d random variables
  exponentially E(lambda_j) distributed and sum them.
  """

  Example:
    var model = function() {
      var lambdas = [1,2,3];
      var g = erlang_dist(lambdas);
      return {
        g:g,
      }
    }
  
  http://www.hakank.org/webppl/erlang_dist.wppl

*/
var erlang_dist = function(lambdas) {
    var s = map(function(lambda) { return exponential_dist(lambda)},lambdas);
    return sum(s);
}


/*
  Exponential distribution

  From Handbook on probability distributions, page 53
  """
  Despite the quantile function is q(u) = -1/lambda * log(1-u),
  generally the exponential distribution E(Î») is generated by applying
  -1/lambda * log(U) on a uniform variate U.
  """

  exponential_dist2 is a variant from
  https://en.wikipedia.org/wiki/Inverse_transform_sampling

  var model = function() {
    var lambda = 1/4;
    var g = exponential(lambda); // built-in
    var g2 = exponential_dist(lambda);
    var g3 = exponential_dist2(lambda);        

    return {
        g:g,
        g2:g2,
        g3:g3,
    }
  }

  http://www.hakank.org/webppl/exponential_dist.wppl

*/
var exponential_dist = function(lambda) {
    var u = uniform(0,1);
    return -1*Math.log(u)/lambda;
}

// From https://en.wikipedia.org/wiki/Exponential_distribution
// Quantile[ExponentialDistribution[lambda], x]
var exponential_dist_quantile = function(lambda, q) {
    return -Math.log(1-q) / lambda
}


// From https://en.wikipedia.org/wiki/Inverse_transform_sampling
var exponential_dist2 = function(lambda) {
    var u = uniform(0,1);
    return (-1/lambda)*Math.log(1-u);
}

var exponential_dist2_quantile = function(lambda,q) {
   return (-1/lambda)*Math.log(1-q);    
}

/*
  Exponential
  https://en.wikipedia.org/wiki/Exponential_distribution
*/
var exponential_pdf = function(lambda,x) {
    return lambda*Math.exp(-lambda*x)
}

var exponential_cdf = function(lambda,x) {
    return 1-Math.exp(-lambda*x)
}


/*
  Inverse Exponential distribution

  From Handbook on probability distributions, page 60
  """
  The algorithm is simply to inverse an exponential variate of parameter
  1/lambda, i.e. (-lambda log(U))-1 for an uniform variable U.
  """

  Example:

  var model = function() {
    var lambda = 1/4;
    
    var g = inverse_exponential_dist(lambda);
    var g2 = exponential(1/lambda); 

    return {
        g:g,
        g2:g2,
    }

  http://www.hakank.org/webppl/exponential_inverse_dist.wppl

*/
var inverse_exponential_dist = function(lambda,t) {
    var u = uniform(0,1);
    return -lambda*Math.log(u);
}

var inverse_exponential_dist_quantile = function(lambda,q) {
    return -lambda*Math.log(q)
}

/*
  Shifted Exponential distribution

  From Handbook on probability distributions, page 60
  """
  The random generation is simple: just add Ï„ to the
  algorithm of exponential distribution
  """

  Example:

  var model = function() {
    var lambda = 1/4;
    var t = 1;
    var g2 = shifted_exponential_dist(lambda,t);    

    return {
        g2:g2,
    }
  }

  http://www.hakank.org/webppl/exponential_shifted_dist.wppl
*/
var shifted_exponential_dist = function(lambda,t) {
    var u = uniform(0,1);
    return -1*Math.log(u)/lambda + t ;
}

var shifted_exponential_dist_quantile = function(lambda,t,q) {
    return -1*Math.log(q)/lambda + t ;    
}


/*
  Gamma distribution

  Note: Only the integer variant is implemented.

  From Handbook on probability distributions
  page 60
  """
  Simulate a gamma G(a, lambda) is quite tricky for non integer shape parameter.
  Indeed, if the shape parameter a is integer, then we simply sum a exponential
  random variables E(lambda). Otherwise we need to add a gamma variable
  G(alpha-abs(alpha), lambda). This is carried out by an acceptance/rejection method.
  """

  Note this only supports integer a.
  And to compare with the built-in gamma function, we inverse the lambda
  parameter.

  Example:

  var model = function() {
    var a = 4;
    var lambda = 1/2;
    
    var g = gamma_int_dist(a, lambda);
    var g2 = gamma(a,lambda); // built-in method

    return {
        g:g,
        g2:g2,
    }
  }

  http://www.hakank.org/webppl/gamma_dist.wppl

*/
var gamma_int_dist = function(a, lambda) {
    var s = mapN(function(i) { return exponential_dist(1/lambda)},a);
    return sum(s);
}


/*
  Generalized (Transformed) Gamma distribution

  From Handbook on probability distributions, page 67
  """
  Generate a gamma distributed variable (G(Î±, 1)), raise it to power 1/t
  and multiply it by lambda
  """

  Example:

  var model = function() {
    var g = generalized_gamma_dist(3,1/2,1/3);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/gamma_generalized_dist.wppl

*/
var generalized_gamma_dist = function(a,lambda,t) {
    var x = lambda*Math.pow(gamma(a,1),1/t);   
    return x;
}


/*
  Inverse Gamma distribution

  Note: I'm not sure if b (lambda) should be inversed as well since
  WebPPL's gamma seems to handle scale, not lambda...

  From Handbook on probability distributions, page 66
  """
  Simply generate a gamma variable G(Î±, 1/lambda) and inverse it.
  """

  Example:

  var model = function() {
    var g = inverse_gamma_dist(2,4);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/gamma_inverse_dist.wppl

*/
var inverse_gamma_dist = function(a,lambda) {
    var x = 1/gamma(a,lambda)
    return x;
}


/*
  
  Inversed transformed Gamma distribution

  From Handbook on probability distributions
  page 68
  """
  Simply simulate a gamma G(a, 1) distributed variable,
  inverse it, raise it to power 1/a [shouldn't it be 1/t?]
  and mutiply it by lambda.
  """

  Example:

  var model = function() {
    var g = inversed_transformed_gamma_dist(3,2,1);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/gamma_inverse_transformed_dist.wppl

*/
var inversed_transformed_gamma_dist = function(a,lambda,t) {
    var x = Math.pow(1/gamma(a,1),1/t)/lambda;
    return x;
}


/*
  Gaussian distribution

  From Handbook on probability distributions
  page 49
  """
  The Box-Muller algorithm produces normal random variates:
  * generate U, V from a uniform U(0, 1) distribution,
  * compute X = sqrt(-2*log(U))*cos(2*Pi*V) and Y = sqrt(-2*log(U))*sin(2*Pi*V ).
  In outputs, X and Y follow a standard normal distribution (independently).
  ...

  But there appears that this algorithm under estimates the tail
  of the distribution (called the Neave effect, cf. Patard (2007)),
  most softwares use the inversion function method, consist in
  computing the quantile function Î¦-1 of a uniform variate. 
  """

  Example
  var model = function() {
    var g = gaussian(0,1); // Built-in version
    var g2 = gaussian01(); // This version
    var mean = 100;
    var std = 10;
    var g3 = gaussian(mean,std); // Built-in version
    var g4 = gaussian_dist(mean,std); // This version
    return {
        g:g,
        g2:g2,
        g3:g3,
        g4:g4,
    }
  }

  http://www.hakank.org/webppl/gaussian_dist.wppl

*/
var gaussian01 = function() {
    var u = uniform(0,1);
    var v = uniform(0,1);
    // var x = Math.sqrt(-2*Math.log(u))*Math.cos(2*Math.PI*v);
    var y = Math.sqrt(-2*Math.log(u))*Math.sin(2*Math.PI*v);

    // return x;
    return y;    
}

var gaussian_dist = function(mean,std) {
    return mean + (gaussian01() * std);
}

// This is not very exact:
// Quantile[NormalDistribution[100, 15], 0.99] -> 134.895
// hakank_utils.normalQuantile(100,15,0.99) -> 134.8613498234336
//
// Quantile[NormalDistribution[100, 15], 0.9999] -> 155.785
// hakank_utils.normalQuantile(100,15,0.9999) -> 155.67772635407104
//
// Quantile[NormalDistribution[100, 15], 0.0001] -> 44.2148
// hakank_utils.normalQuantile(100, 15, 0.0001) -> 44.322273645928966
//
var gaussian_dist_quantile = function(mean,std, q) {
    return hakank_utils.normal_quantileJS(mean, std, q)
}

/*
  Gaussian PDF
  https://en.wikipedia.org/wiki/Normal_distribution
*/
var gaussian_pdf = function(mu,sigma,x) {
    return Math.exp((-1/2) * Math.pow((x-mu)/sigma,2) ) / (sigma*Math.sqrt(2*Math.PI)) 
}

/*  
  Gaussian CDF
*/
var gaussian_cdf = function(mu,sigma,x) {
    return hakank_utils.normal_cdfJS(mu,sigma,x)
}


/*
  Geometric distribution

  From Handbook on probability distributions
  page 19
  Expectation: (1-p)/p
  """
  A basic algorithm is to use i.i.d. Bernoulli variables as follows:
  * initialize X to 0 and generate U from an uniform distribution,
  * while U > p do ; generate U from an uniform distribution; X = X + 1;
  * return X.
  """
  
  Example:

  var model = function() {
    var p = 0.9;
    var g = geometric_dist(p);
    return {
        g:g
    }
  }

*/
var geometric1 = function(p,x) {
    var u = uniform(0,1);
    return u > p ? geometric1(p,x+1) : x;
}

var geometric_dist = function(p) {
    return geometric1(p,0);
}

var geometric_dist_quantile = function(p,q) {
    return hakank_utils.geometric_quantileJS(p,q);
}

/*
  Geometric distribution
  https://en.wikipedia.org/wiki/Geometric_distribution  
*/
var geometric_pdf = function(p,k) {
    return Math.pow(1-p,k)*p // Math.pow(1-p,k-1)*p
}

var geometric_cdf = function(p,k) {
    return sum(mapN(function(r) { geometric_pdf(p,r) }, k+1))
}


/*
  Exact gemetric distribution.
  "Exact" in the sense that it can be calculated by the "enumerate" solver.

  From http://dippl.org/chapters/02-webppl.html

*/
var geometric_exact_dist = function(p) {
    // return flip(p) ? 1 + geometric(p) : 1
    return flip(p) ? 0 : 1 + geometric_exact_dist(p)
}


/*
  Zero Modified (Inflated) Geometric distribution

  From Handbook on probability distributions
  page 21
  """
  While for the zero-modified geometric distribution, it is a little bit tricky
  * generate U from an uniform distribution
  * if U < p, then X = 0
  * otherwise
    - initialize X to 1 and generate U from an uniform distribution
    - while U > q do ; generate U from an uniform distribution; X = X + 1;
  * return X
  """

  Example:
  var model = function() {
    var p = 0.9; // Probability of zero
    var q = 0.4;

    var g = zero_inflated_geometric_dist(p,q);
    
    return {
        g:g
    }
  }

  http://www.hakank.org/webppl/geometric_zero_modified_dist.wppl

*/
var zero_inflated_geometric1 = function(p,q,x) {
    var u = uniform(0,1);    
    return u > q ? zero_inflated_geometric1(p,q,x+1) : x;
}

// p is the probability of 0
var zero_inflated_geometric_dist = function(p,q) {
    var u = uniform(0,1);
    if (u < p) {
        return 0;
    } else {
        return zero_inflated_geometric1(p,q,1);
    }
}


/*
  Zero truncated Geometric distribution

  From Handbook on probability distributions, page 21
  Zero truncated Geometric distribution is a Geometric distribution 
  but zero is not a possible value.
   
  It's used for generating a Pascal distribution, see pascal_dist

  Example:

  var model = function() {
    var p = 0.9;
    var g = geometric_zero_truncated_dist(p);
    return {
        g:g
    }
  }
 
  http://www.hakank.org/webppl/geometric_zero_truncated_dist.wppl

*/
var geometric_zero_truncated1 = function(p,n) {
    var u = uniform(0,1);
    return u > p ? geometric_zero_truncated1(p,n+1) : n;
}

var geometric_zero_truncated_dist = function(p) {
    return geometric_zero_truncated1(p,1);
}


/*
  Hypergeometric distribution

  https://en.wikipedia.org/wiki/Hypergeometric_distribution
  """
  T]he probability of k successes (random draws for which the object 
  drawn has a specified feature) in n draws, without replacement, from 
  a finite population of size N that contains exactly K objects with 
  that feature, wherein each draw is either a success or a failure. 
  In contrast, the binomial distribution describes the probability of 
  k successes in n draws with replacement. 
  """

  Cf https://github.com/distributions-io/hypergeometric-random/blob/master/lib/number.js

  Hypergeometric:
  What is the probability that we draw exactly k "success" objects
  of the n drawn objects of total N objects where there are in total
  K "success" objects

  k: number of successes we want to check
  N: total number of objects
  K: total number of success objects
  n: number of draws

  Here are two versions:
  - hypergeometric_dist(k,N,K,n):
    Probability of exactly k successes.

  - hypergeometric_count(k,N,K,n)
    The number of found successes

  Example:

  var model = function() {

    // total: 5 green and 45 red marbles
    // drawn: 4 green marbles, 6 red marbles
    var K = 5; // total green marbles: 4 drawn + 1 not drawn
    var N = 50; // total marbles: 5 green + 45 red marbles
    
    var k = 4; // drawn green_marbles
    // var k = 5; // drawn green_marbles    
    var n = 10 // total drawn green + red marbles
    
    var g = hypergeometric(k,N,K,n);
    // var g = hypergeometric_count(k,N,K,n); // Count version
    
    return {
        g:g
    }
    
  }

  http://www.hakank.org/webppl/hypergeometric_dist.wppl  

*/
var hypergeometric1 = function(k,N,K,n,count) {
    if (n==0 || K<= 0) {
        return count;
    } else {
        // we have K successes left and N objects left
        var p = K/N; // probability of drawing a success object
        if (flip(p)) {
            // We drew a success:
            // - decrement the total objects (N)
            // - decrement the number of "success" objects (K)
            // - decrement the number of drawn objects (n)
            // - increment the number of successful draws (count)
            return hypergeometric1(k,N-1,K-1,n-1,count+1);
        } else {
            // We drew a failure:
            // - decrement the total objects (N)
            // - decrement the number of drawn objects (n)
            return hypergeometric1(k,N-1,K,n-1,count);
        }
    }
}

var hypergeometric_dist = function(k,N,K,n) {
    var res = hypergeometric1(k,N,K,n,0);
    return res == k;
}

// Return the number of found successes.
var hypergeometric_count = function(k,N,K,n) {
    var res = hypergeometric1(k,N,K,n,0);
    return res;
}
/*  
   Hypergeometric 
   From https://en.wikipedia.org/wiki/Hypergeometric_distribution
*/
// Same order of arguments as hypergeometric_count (except that k is last)
var hypergeometric_pdf = function(N,K,n,k) {
    return binomialf(K,k)*binomialf(N-K,n-k)/binomialf(N,n)
}

var hypergeometric_cdf = function(N,K,n,k) {
    return sum(mapN(function(r) {
        hypergeometric_pdf(N,K,n,r)
    }, k+1))
}


var hypergeometric_dist_quantile = function(N,K,n,q) {
    // Reversing the CDF.
    var t = mapN(function(i) {
        var x = hypergeometric_cdf(N,K,n,i);
        return [i,x >= q]
    },N+1);
    var tt = filter(function(x) { x[1] == true  },t)
    return first(tt)[0]
    
}

/*
  This mirrors Mathematica's version of Hypergeometric[n,n_succ,n_tot]
  """
  A hypergeometric distribution gives the distribution of the number of successes in 
  n draws from a population of size n_tot containing n_succ successes.
  """

  It uses hypergeometric_count(k,N,k,n)
  k:number of successes we want to check (not used)
  N: total number of objects
  K: total number of success objects
  n: number of draws

*/
var hypergeometric2_dist = function(n_draws, n_succ, n_tot) {
    return hypergeometric_count(1,n_tot,n_succ,n_draws)
}

// Mathematica style (same order of arguments as hypergeometric2_dist)
var hypergeometric2_pdf = function(n_draws,n_spec,n_tot,k) {
    return hypergeometric_pdf(n_tot,n_spec,n_draws,k)
}

// CDF
var hypergeometric2_cdf = function(n_draws,n_spec,n_tot,k) {
    return sum(mapN(function(r) {
        hypergeometric2_pdf(n_draws,n_spec,n_tot,r)
    }, k+1))
}

var hypergeometric2_dist_quantile = function(n_draws,n_spec,n_tot,q) {
    // Reversing the CDF.
    var t = mapN(function(i) {
        var x = hypergeometric2_cdf(n_draws,n_spec,n_tot,i);
        return [i,x >= q]
    },n_draws+1);
    var tt = filter(function(x) { x[1] == true  },t)
    return first(tt)[0]

}


/*
  Laplace distribution

  From Handbook on probability distributions
  page 73
  """
  Let U be a uniform variate. Then the algorithm is
  * V = U - 1/2
  * X = m + sigma*sign(V ) log(1 - 2|V|)
  return X
  """

  Example:
  var model = function() {
    var mu = 0;
    var sigma = 1;
    var g = laplace_dist(mu,sigma);
    var g2 = laplace(mu,sigma); // built-in

    return {
        g:g,
        g2:g2,

    }
  }

  http://www.hakank.org/webppl/laplace_dist.wppl
  
*/
var laplace_dist = function(mu,sigma) {
    var u = uniform(0,1);
    var v = u-1/2;
    var x = mu + sigma*Math.sign(v)*Math.log(1-2*Math.abs(v));
    
    return x;
}

// Quantile[LaplaceDistribution[mu, sigma], q]
var laplace_dist_quantile = function(mu,sigma,q) {
    return q <= 1/2 ? mu + sigma*Math.log(2*q) : mu - sigma*Math.log(2*(1-q))
}

/*
  Laplace
  https://en.wikipedia.org/wiki/Laplace_distribution
*/
var laplace_pdf = function(mu,b,x) {
    return Math.exp(-Math.abs(x-mu)/b)/(2*b)
}


/*
  Log Gamma distribution

  From Handbook on probability distributions, page 69
  """
  Simply simulate a gamma G(k, 1) distributed variable and returns a + b log(X).
  """

  Example:
  var model = function() {
     
    var g = log_gamma_dist(3,2,1);

    return {
        g:g,
    }
  }
  
  http://www.hakank.org/webppl/log_gamma_dist.wppl

*/
var log_gamma_dist = function(k,a,b) {    
    return a+b*Math.log(gamma(k,1));
}


/*
  LogNormal distribution

  From Handbook on probability distributions, page 51
  """
  Once we have generated a normal variate, it is easy to generate
  a log-normal variate just by taking the exponential of normal
  variates.
  """

  Example:
  var model = function() {   
    var g = log_normal_dist(0,1);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/log_normal_dist.wppl

*/
var log_normal_dist = function(mu,sigma) {
    var g = gaussian(mu,sigma);
    return Math.exp(g);    
}


/*
  Shifted LogNormal distribution

  From Handbook on probability distributions, page 53
  """
  Once we have generated a normal variate, it is easy to generate a
  log-normal variate just by taking the exponential of normal variates and
  adding the shifted parameter Î½.
  """

  Example:

  var model = function() {
     
    var g = shifted_log_normal_dist(0,1,4);

    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/log_normal_shifted_dist.wppl

*/
var shifted_log_normal_dist = function(mu,sigma,v) {
    var g = gaussian(mu,sigma);
    return Math.exp(g) + v;    
}


/*
  Negative Binomial distribution
  
  From Handbook on probability distributions, page 22ff
  """
  The algorithm to simulate a negative binomial distribution NB(m, p) 
  is simply to generate m random variables geometrically distributed 
  and to sum them.
  """

  Example:
  var model = function() {
    var d = negative_binomial_dist(3,0.7);
    return {
        d:d,
        p:(d >= 4),
    }
  }

  http://www.hakank.org/webppl/negative_binomial_dist.wppl

*/
var negative_binomial_dist = function(m,p) {
    var g = mapN(function() { return geometric_dist(p)}, m);
    return sum(g);
}

var negative_binomial_dist_quantile = function(m,p,q) {
    return hakank_utils.negative_binomial_quantileJS(m,p,q)
}

/*
  Negative binomial 
  https://en.wikipedia.org/wiki/Negative_binomial_distribution

*/
var negative_binomial_pdf = function(r,p,k) {
    return binomialf(k+r-1,k)*Math.pow(1-p,k)*Math.pow(p,r)
}

/*
  negative_binomial_cdf(n,p,k) TODO!
  Requires regularized incomplete beta function

  Mathematia:
  CDF[NegativeBinomialDistribution[n, p], k]
  -> 
  BetaRegularized[p, n, 1 + Floor[k]]

*/
var negative_binomial_cdf = function(n,p,k) {
    return sum(mapN(function(r) {
        negative_binomial_pdf(n,p,r)
    },k+1))
}

/*
  Negative binomial 

  Alternative implementation, not using geometric_dist which uses uniform(0,1).
  This means that we can use enumerate for this. However, it should probably
  be restricted with maxExecutions, e.g.
    
     var d = Infer({method:"enumerate",maxExecutions:1000},model)

  Example
  var model = function() {
    var y = negative_binomial(3,0.7)  
    return {y:y,
            p:y==2
           }
    }

  See http://hakank.org/webppl/negative_binomial_test.wppl

*/
var negative_binomial_exact_dist = function(m,p) {
    var g = mapN(function() { return geometric_exact_dist(p)}, m);
    return sum(g);
}


/*
  Pareto distribution

  See https://en.wikipedia.org/wiki/Pareto_distribution
  and 
  https://math.stackexchange.com/questions/1777367/how-to-generate-a-random-number-from-a-pareto-distribution

  Example:

  var model = function() {
    var shape = 4; // a
    var scale = 6820; // b
    var p = pareto_dist(shape,scale);
    return {
        p:p
    }
  }

  http://www.hakank.org/webppl/pareto_dist.wppl

  Note: What I can see, Mathematica has the parameters in reverse order:
  ParetoDistribution[scale,shape]
*/
var pareto_dist = function(shape,scale) {
    var u = uniform(0,1);
    var x = scale/Math.pow(1-u,1/shape)
    return x;
}

// 
var pareto_dist_quantile = function(shape,scale,q) {
    return scale/Math.pow(1-q,1/shape)    
}

/*
  Pareto distribuitions, types I..IV

  From Handbook on probability distributions, page 95
  """
  ... Therefore algorithms for random generation are simply
  * for P_I(sigma, alpha) distribution, F^-1(u)= sigma*U^(-1/alpha)
  * for P_II(mu, sigma, alpha) distribution, F^-11(u) = sigma*(U^(-1/alpha - 1) + mu,
  * for P_III(mu, sigma, gamma) distribution, F^-1(u) = sigma*(U^(-1)-1)^gamma + mu,
  * for P_IV(mu, sigma, alpha[, gamma]) distribution, F^-1(u) = sigma*(U^(-1/alpha)-1)^gamma + mu,
  where U is an uniform random variate.
  """
  Note: P_IV is missing the fourth parameter gamma in the text.


  Example:
  var model = function() {
    var g1 = pareto_i_dist(2,3);
    var g2 = pareto_ii_dist(2,3,1);
    var g3 = pareto_iii_dist(1,1,1);
    var g4 = pareto_iv_dist(0,1,1,2);
    return {
        g1:g1,
        g2:g2,
        g3:g3,
        g4:g4,
    }
  }

  http://www.hakank.org/webppl/pareto_dist2.wppl

  Note: These distributions are not compliant with Mathematica (and I'm not sure why).

*/
var pareto_i_dist = function(sigma,alpha) {
    var u = uniform(0,1); 
    return sigma*Math.pow(u,(-1/alpha));
}

var pareto_i_dist_quantile = function(sigma,alpha, q) {
    return sigma*Math.pow(q,(-1/alpha));
}


var pareto_ii_dist = function(mu,sigma,alpha) {
    var u = uniform(0,1);
    return sigma*(Math.pow(u,-1/alpha)-1) + mu;
}

var pareto_ii_dist_quantile = function(mu,sigma,alpha,q) {
    return sigma*(Math.pow(q,-1/alpha)-1) + mu;
}

var pareto_iii_dist = function(mu,sigma,gammav) {
    var u = uniform(0,1); 
    return sigma*Math.pow(Math.pow(u,-1)-1,gammav) + mu;
}

var pareto_iii_dist_quantile = function(mu,sigma,gammav,q) {
    return sigma*Math.pow(Math.pow(q,-1)-1,gammav) + mu;
}

var pareto_iv_dist = function(mu,sigma,alpha,gammav) {
    var u = uniform(0,1); 
    return sigma*Math.pow(Math.pow(u,-1/alpha)-1,gammav) + mu;
}

var pareto_iv_dist_quantile = function(mu,sigma,alpha,gammav,q) {
    return sigma*Math.pow(Math.pow(q,-1/alpha)-1,gammav) + mu;
}

/*
   The pareto_<i|ii|iii|iv>_dist2 are derived from Mathematica's versions
   using Quantile to get the simulation/quantile.
   They have different parameters (and order of the parameters) than
   pareto_<i|ii|iii|iv>_dist.

*/
var pareto_i_dist2 = function(k,alpha) {
    var u = uniform(0,1)
    return k*Math.pow(1-u,-1/alpha)
}

var pareto_i_dist2_quantile = function(k,alpha,q) {
    return k*Math.pow(1-q,-1/alpha)
}

var pareto_ii_dist2 = function(k,alpha,mu) {
    var u = uniform(0,1)
    return mu + k*(-1 + Math.pow(1-u,-1/alpha))
}

var pareto_ii_dist2_quantile = function(k,alpha,mu,q) {
    return mu + k*(-1 + Math.pow(1-q,-1/alpha))
}

var pareto_iii_dist2 = function(k,gamma,mu) {
    return pareto_iv_dist2(k,1,gamma,mu)
}

var pareto_iii_dist2_quantile = function(k,gamma,mu,q) {
    return mu + k * Math.pow(-1 + 1/(1-q),gamma)
}

// This variant is from Mathematica's Quantile function
// Quantile[ParetoDistribution[k,alpha,gamma,mu],x]
// -> mu + k * (-1 + (1-q)^(-1/alpha)^gamma
var pareto_iv_dist2 = function(k,alpha,gamma,mu) {
    var u = uniform(0,1)
    return mu + k * Math.pow(-1 + Math.pow(1-u,-1/alpha),gamma)
}

var pareto_iv_dist2_quantile = function(k,alpha,gamma,mu,q) {
    return mu + k * Math.pow(-1 + Math.pow(1-q,-1/alpha),gamma)
}


/*
  Generalized Pareto distribution

  From Handbook on probability distributions
  page 101
  """
  We have an explicit expression for the quantile function
  F^-1(u) =
     if xi != 0:
        eta + (sigma/xi)*( (1-u)^(-xi) - 1)
     if xi == 0:
        eta - sigma*log(1-u)
  thus we can use the inversion function method to generate
  GPD variables.
  """

  Example:
  var model = function() {
    var g = generalized_pareto_dist(1,1,-5/4);  
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/pareto_generalized_dist.wppl
  
*/
var generalized_pareto_dist = function(eta,sigma,xi) {
    var u = uniform(0,1);
    if (xi != 0) {
        return eta + (sigma/xi)*( Math.pow(u,-xi) - 1);
    } else {
        return eta + sigma*Math.log(u);
    }
}


/*
  Inverse Pareto distribtion

  From Handbook on probability distributions, page 98
  """
  Simply inverse a Pareto II variable.
  """

  Example:
  var model = function() {
    var g = inverse_pareto_dist(2,1,1);  
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/pareto_inverse_dist.wppl

*/
var inverse_pareto_dist = function(mu,sigma,alpha) {
    return 1/pareto_ii_dist(mu,sigma,alpha);
}

/*
  Pascal distribution

  From Handbook on probability distributions, page 25
  """
  The negative binomial distribution can be constructed by summing 
  m geometric distributed variables G(p). The Pascal distribution is 
  got from summing n geometrically distributed G0(p) variables.
  Thus possible values of the Pascal distribution are in {n, n+ 1, ...}.

  ...

  The link between Pascal distribution Pa(n,p) and the negative 
  binomial distribution BN(n,p) is to substract the constant n, i.e. 
  if X ~ Pa(n,p) then X-n ~ BN(n, p).
  """

  Example:
  var model = function() {
    // Cf negative_binomial_test.wppl
    var d = pascal(3,0.5);
    var p = d <= 6
    return {
        d:d,
        p:p,
    }
  }

  http://www.hakank.org/webppl/pascal_dist.wppl

*/
var pascal_dist = function(m,p) {
    var g = mapN(function() { return geometric_zero_truncated_dist(p)}, m);
    return sum(g);
}


/*
  Poisson distribution

  From Handbook on probability distributions, page 14
  """
  A basic way to generate Poisson random variate is the following:
  * initialize variable n to 0, l to exp(-lambda) and P to 1,
  * do
    - generate U from a uniform distribution,
    - P = P * U,
    - n = n 0 1,
    while P >= l,
  return n - 1.
  See Knuth (2002) for details.
  """
  Note: n is the counter, p2 is the acculumated value

  Example:
  var model = function() {
    var n = 4;   
    var p = poisson(n); // built-in
    var p2 = poisson_dist(n);
    return {
        p:p,
        p2:p2
    }
  }

  http://www.hakank.org/webppl/poisson_dist.wppl

*/
// Note: n is the counter, p2 is the acculumated value
var poisson_dist1 = function(lambda,n,p2) {
    var l = Math.exp(-lambda);
    var u = uniform(0,1);
    var p = p2*u;
    return p >= l ? poisson_dist1(lambda,n+1,p) : n;
}

var poisson_dist = function(lambda) {
    return poisson_dist1(lambda,0,1);
}

//
// poisson_quantileJS(lambda,q) is defined in hakank_utils.js
// (from ChatGPT-3.5)
//
var poisson_dist_quantile = function(lambda,q) {
    return hakank_utils.poisson_quantileJS(lambda,q)
}

/*
  Poisson
  https://en.wikipedia.org/wiki/Poisson_distribution
*/
var poisson_pdf = function(lambda,k) {
    return Math.pow(lambda,k)*Math.exp(-lambda)/factorial(k)
}

// 
var poisson_cdf = function(lambda,k) {
    return sum(mapN(function(r) { poisson_pdf(lambda,r)}, k+1))
}

/*
  Zero Modified Poisson distribution

  
  From Handbook on probability distributions, page 17
  """
  The zero-modified version P(lambda, p) is a little bit tricky.
  We need to use the following heuristic:
  * generate U from an uniform distribution
  * if U < p, then X = 0
  * otherwise
    - do; generate X Poisson distributed P(lambda); while X = 0
  * return X
  """

  Example:
  var model = function() {

    var lambda = 4;
    var p = 0.2;
    var p = zero_modified_poisson_dist(lambda,p);

    return {
        p:p,
    }
  }

  http://www.hakank.org/webppl/poisson_zero_modified_dist.wppl

*/
var zero_modified_poisson_dist = function(lambda,p) {
    var u = uniform(0,1);
    if (u < p) {
        return 0;
    } else {
        var p2 = poisson(lambda);
        return p2 != 0 ? p2 : zero_modified_poisson_dist(lambda,p);
    }
}


/*
  Zero Truncated Poisson distribution

  Algorithm from Handbook on probability distributions,
  page 16
  """
  The basic algorithm for the zero-truncated version P0(lambda) is simply
  * do; generate X Poisson distributed P(Î»); while X = 0
  * return X
//
  In output, we have a random variate in N âˆ— .
  The zero-modified version P(Î», p) is a little bit tricky. We need to use the following heuristic:
  - generate U from an uniform distribution
  - if U < p, then X = 0
  - otherwise
  - do; generate X Poisson distributed P(Î»); while X = 0
  - return X
  """

  Example:
  var model = function() {

    var lambda = 4;
    
    var p = zero_truncated_poisson_dist(lambda);

    return {
        p:p,
    }
  }

  http://www.hakank.org/webppl/poisson_zero_truncated_dist.wppl
  
*/
// Note: n is the counter, p2 is the acculumated value
var zero_truncated_poisson_dist = function(lambda) {
    var p = poisson(lambda);
    return p != 0 ? p : zero_truncated_poisson_dist(lambda);
}


/*
  Student-T distribution

  From Handbook on probability distributions
  page 85
  """
  The algorithm is simply
  * generate a standard normal distribution N
  * generate a chi-squared distribution C
  * return sqrt(d)*N/sqrt(C)
  """

  Example:
  var model = function() {
    var d = 4;
    var g = student_t_dist(d);

    return {
        g:g,

    }
  }
  
  http://www.hakank.org/webppl/student_t_dist.wppl

*/
var student_t_dist = function(d) {
    var N = gaussian(0,1);
    var C = chi_squared_dist(d);
    return Math.sqrt(d)*N/Math.sqrt(C);
}


/*
  Triangular distribution

  From Handbook on probability distributions, page 38
  """
  Expectation: (a+b+c)/3.
  """

  Example:
  var model = function() {
    var a = 0;
    var b = 3;
    var c = 10;
    var g = triangular_dist(a,b,c);
    return {
        g:g
    }
  }

  http://www.hakank.org/webppl/triangular_dist.wppl

*/
var triangular_dist = function(a,b,c) {
    var u = uniform(0,1);
    var v = uniform(0,1);
    var cc = (c-a)/(b-a);
    var x = a + (b-a) * ((1-cc)*Math.min(u,v) + cc*Math.max(u,v));
    return x;
}


/*
  Discrete Uniform distribution

  From Handbook on probability distributions
  page 8
  """
  The algorithm is simply:
  - generate U from a uniform distribution
  - compute the generated index as I = ceiling(n*U)
  - finally X is k[I]
  """

  Example:
  var model = function() {
    var a = _.range(10);
    var g = uniform_discrete_dist(a);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/uniform_discrete_dist.wppl

*/
var uniform_discrete_dist = function(a) {
    var n = a.length;
    var u = uniform(0,1);
    var ix = Math.ceil(n*u)-1; // Adjust for base 0
    return a[ix];
}

/*
  Uniform
  https://en.wikipedia.org/wiki/Continuous_uniform_distribution
*/

var uniform_pdf = function(a,b,x) {
    return x >= a && x <= b ? 1/(b-a) : 0
}

/*
  Weilbull distribution


  From Handbook on probability distributions
  page 79
  """
  Using the inversion function method, we simply need to compute
  beta(-log(1-U))^1/eta) for the first parametrization or
  (-log(1-U)/t)^(1/eta) for the second one where U is an
  uniform variate.
  """

  Example:
  var model = function() {
    var g = weibull_dist(4,3);
    return {
        g:g,
    }
  }

  http://www.hakank.org/webppl/weibull_dist.wppl

*/
var weibull_dist = function(eta,beta) {
    var u = uniform(0,1);
    var x = beta*Math.pow(-Math.log(1-u),1/eta)
    
    return x;
}

var weibull_dist_quantile = function(eta,beta,q) {
    return beta*Math.pow(-Math.log(1-q),1/eta)
}




/*
  
  Inverse Weibull distribution

  From Handbook on probability distributions, page 72
  """
  Simply generate a Weibull variable W(1/beta,eta) and inverse it.
  """
  Note: I'm a little confused here since the weibull dist is noted as
  W(eta,beta) according to the Weibull Distribution entry.
  
  Example:
  var model = function() {

    var eta = 4;
    var beta = 2;
    var g = inverse_weibull_dist(beta,eta);
    var expect = inverse_weibull_expectation(beta,eta);

    return {
        g:g,
        expect:expect,
    }
  }

  http://www.hakank.org/webppl/weibull_inverse_dist.wppl


*/
// From https://stackoverflow.com/questions/15454183/how-to-make-a-function-that-computes-the-factorial-for-numbers-with-decimals
// This is to calculate the expectation
// (factorial(z) ~ gamma(z+1)
var gamma_func = function(z) {
  return Math.sqrt(2*Math.PI/z)*Math.pow((1/Math.E)*(z+1/(12*z-1/(10*z))), z);
}

var inverse_weibull_dist = function(beta,eta) {
    var x = 1/weibull_dist(eta,1/beta);
    
    return x;
}

var inverse_weibull_expectation = function(beta,eta) {
    return beta*gamma_func(1-1/eta);
}


/*
  Gumbel distribution


  From Handbook on probability distributions
  page 112
  """
  The quantile function of the Gumbel I distribution is simply
  F^-1(u) = mu − sigma log(- log(u)), thus
  we can use the inverse function method.

  The expectation of a Gumbel type I distribution is E(X) = gamma, the
  Euler constant, roughly 0.57721.
  Its variance is Var(X) = pi^2*6 . Thus for the Fisher-Tippett distribution,
  we have E(X) = mu + sigma*gamma and Var(X) = pi^2sigma^2 / 6 .
  """

  Example:
  var model = function() {
     
    var g = gumbel_dist(0,1)
    // var g = gumbel_dist(1/2,1)
    // var g = gumbel_dist(1/2,1)
    // var g = gumbel_dist(1,2)        

    return {
        g:g,
    }
  }

  See http://hakank.org/webppl/gumbel_dist.wppl

*/
var gumbel_dist = function(mu,sigma) {
    var u = uniform(0,1);
    var x = mu - sigma*Math.log(-Math.log(u))
    return x;
}

// Quantile[GumbelDistribution[mu, sigma], q]
var gumbel_dist_quantile = function(mu,sigma,q) {
    return mu + sigma*Math.log(-Math.log(1-q))
}


/*
  Gumbel
  https://en.wikipedia.org/wiki/Gumbel_distribution

*/
var gumbel_pdf = function(mu,sigma,x) {
    var z = (x-mu)/sigma
    return Math.exp(-Math.exp(z)+z)/sigma
}


/*
  Kumaraswamy distribution

  From Handbook on probability distributions
  """
  Since the quantile function is explicit
     F^-1(u) = (1 - (1 - u)^(1/b))^(1/a)
  an inversion function method F^-1(u) with u uniformly distributed is easily computable.
  """

  Example
  var model = function() {
    var a = 5
    var b = 2
    var g = kumaraswamy_dist(a,b)
    return {
        a:a,
    }
  }

  http://hakank.org/webppl/kumaraswamy_dist.wppl
  
*/
var kumaraswamy_dist = function(a,b) {
    var u = uniform(0,1)
    return Math.pow(1-Math.pow(1-u,1/b),1/a)
}


var kumaraswamy_dist_quantile = function(a,b,q) {
    return Math.pow(1-Math.pow(1-q,1/b),1/a)    
}

/*

  Logistic distribution

  From https://en.wikipedia.org/wiki/Logistic_distribution
  """
  Quantile function
  The inverse cumulative distribution function (quantile function) of the 
  logistic distribution is a generalization of the logit function. Its derivative is 
  called the quantile density function. They are defined as follows:
    Q(p;mu,s) = mu + s * ln(p/(1-p))
    Q(p;s)    = s/(p*(1-p))
  """

  http://hakank.org/webppl/logistic_dist.wppl

*/
var logistic_dist = function(mu,s) {
    var u = beta(1,1)
    return mu + s * Math.log(u/(1-u))
}

var logistic_dist_quantile = function(mu,s,q) {
   return mu + s * Math.log(q/(1-q))    
}

var logistic1_dist = function(s) {
    return s/(u*(1-u))
}

var logistic1_dist_quantile = function(s,q) {
    return s/(q*(1-q))
}

/*
  Logistic
  https://en.wikipedia.org/wiki/Logistic_distribution
*/

var logistic_pdf = function(mu,s,x) {
    return Math.exp(-(x-mu)/s)/(s*Math.pow(1+Math.exp(-(x-mu)/s),2))
}


/*
   Generating Extreme value distribution

   From https://www.randomservices.org/random/special/ExtremeValue.html
   """
   The distribution is also known as the standard Gumbel distribution in honor of Emil Gumbel. 
   As we will show below in [13], it arises as the limit of the maximum of independent random 
   variables, each with the standard exponential distribution (when this maximum is 
   appropriately centered). This fact is the main reason that the distribution is special, 
   and is the reason for the name. For the remainder of this discussion, suppose that random variable 
   has the standard Gumbel distribution.

   ...

   The quantile function G^-1 of V given by 
     G^-1(p) = -ln(-ln(p))  p in 0..1
   """

   This is implemented as extreme_value_dist1.

   This seems to be about the same as Mathematica's ExtremeValueDistribution[0,1]

   From Mathematica ExtremeValueDistribution
   """
   ExtremeValueDistribution[alpha,beta]
   represents an extreme value distribution with location parameter alpha and scale parameter beta

   --- 

   The extreme value distribution gives the asymptotic distribution of the maximum value in a sample from a distribution such as the normal distribution.

   ...
   
   Quantile[ExtremeValueDistribution[a,b], x]
   -> 
   a-b Log[-Log[x]]    0 < x < 1
   -Infinity            x <= 0   if 0 <= x <= 1
   Infinity             True
   """

   This is implemented as extreme_value_dist2.

   Note that neither of these are strong enough to do parameter recovery...

*/

var extreme_value_dist1 = function() {
    var u = uniform(0,1)
    var x = -Math.log(-Math.log(u))
    return x
}

var extreme_value_dist1_quantile = function(q) {
    return -Math.log(-Math.log(q))
}


var extreme_value_dist2 = function(a,b) {
    var u = uniform(0,1)
    var x = a-b*Math.log(-Math.log(u))
    return x
}

// From Mathematica Quantile[ExtremeValueDistribution[a, b, x] w
var extreme_value_dist2_quantile = function(a,b,q) {
    return a-b*Math.log(-Math.log(q))
}


/*
   From Mathematica (FrechetDistribution)
   """
   Quantile[FrechetDistribution[alpha, beta], x]
   -> 
   beta*(-Log(x))^(-1/alpha)    0 < x < 1
   0                             x <= 0
   Infinity                      True
   """
*/
var frechet_dist = function(alpha,beta) {
    var u = uniform(0,1)
    var x = beta*Math.pow(-Math.log(u),-1/alpha)
    return x
}

// Quantile[FrechetDistribution[alpha, beta, x]
var frechet_dist_quantile = function(alpha,beta,q) {
    return beta*Math.pow(-Math.log(q),-1/alpha)
}


/*
   From Handbook on probability distributions,
   page 113ff
   """
   xi the shape parameter, mu the location parameter and sigma > 0 the scale parameter.

   The quantile function of the generalized extreme value distribution 
   is F^-1(u) = mu + sigma/xi*((-log u)^-xi)-1 
   for xi != 0. So we can use the inverse function method.
   """

   xi: shape (!= 0)
   mu location parameter
   sigma: scale (> 0) 
*/
var generalized_extreme_value_dist = function(xi,mu,sigma) {
    var u = uniform(0,1)
    return mu + (sigma/xi)*Math.pow(-Math.log(u),-xi)-1 
}

var generalized_extreme_value_dist_quantile = function(xi,mu,sigma,q) {
    return mu + (sigma/xi)*Math.pow(-Math.log(q),-xi)-1     
}


// From Mathematica Quantile[UniformDistribution[{a, b}], x]
var uniform_dist_quantile = function(a,b,q) {
    a*(1-q) + b*q
}

/*
  Generating Beta-binomial distribution

  https://en.wikipedia.org/wiki/Beta-binomial_distribution#Generating_beta_binomial-distributed_random_variables
  """
  To draw a beta-binomial random variate X ~ BetaBin(n,a,b) 
  simply draw p ~ Beta(a,b) and then draw X ~ B(n,p)
  """
*/
var beta_binomial_dist = function(n,a,b) {
    var p = beta(a,b)
    var x = binomial(p,n)
    return x
}

/*
  beta_binomial_pdf(n,a,b,x) 

  Example: beta_binomial_pdf(12,10,10,4): 0.1231688303313781

  It's seems to work only for integer values of a and b (probabily due
  to the definition of gamma in betaf() )

*/
var beta_binomial_pdf = function(n,a,b,k) {
    return binomialf(n,k)*betaf(k+a,n-k+b)/betaf(a,b)
}

/*
  beta_binomial_cdf(n,a,b,k)
*/
var beta_binomial_cdf = function(n,a,b,k) {
    return sum(mapN(function(r) {
        beta_binomial_pdf(n,a,b,r)
    }, k+1))
}

/*
  Pólya Distribution

  https://www.randomservices.org/random/urn/Polya.html
  """
  An urn initially contains a red and b green balls, where a and b  are positive integers. 
  At each discrete time (trial), a ball is selected from the urn and then returned to the urn 
  along with c new balls of the same color. The random process is known as Pólya's urn process, 
  named for George Pólya.

  ...

  In terms of the colors of the selected balls, Pólya's urn scheme generalizes the standard 
  models of sampling with and without replacement.

  c = 0: corresponds to sampling with replacement.
  c = -1: corresponds to sampling without replacement.
  """
*/
var polya_dist = function(n, a, b, c) {
    return beta_binomial_dist(n,a/c,b/c)    
}


/*
  From Mathematica BetaBinomialDistribution:
  """
  The distribution models an urn scheme. An urn contains w white balls and b black balls. 
  When a ball is drawn it is returned to the urn together with c additional balls of 
  the same color. The distribution gives the probability of drawing k white balls in n draws.
  """
*/ 
var polya_eggenberg_dist = function(n,w,b,c) {
    return beta_binomial_dist(n,w/c,b/c)
}


/*
  From Mathematica BetaBinomialDistribution
  """
  Find the probability that b black balls were sampled without replacement before a w'th 
  white ball was drawn from an urn initially filled with k_b black and  k_w white balls:
  """
*/
var negative_hypergeometric_dist = function(w,wtot,btot) {
    return beta_binomial_dist(btot,w,wtot-w+1)
}


/*
  Power distribution
  From Mathematica PowerDistribution
  
*/
var power_dist = function(k,a) {
    var u = uniform(0,1)
    return Math.pow(u,1/a)/k
}

var power_dist_quantile = function(k,a,q) {
    return Math.pow(q,1/a)/k
}

/*
  Zipf distribution https://en.wikipedia.org/wiki/Zipf%27s_law
  and from Mathematica's ZipfDistribution

  zipf_dist(n,s)

  Note that this is the bounded version of Zipf distribution.
  The "standard" version zipf(s) is where n->Infinity, thus it's 
  not supported here... 

  This is also known as the Estoup distribution.
*/

// Generate a random number between 1 and n according to the probability
// in the PDF
// This can be used for exact probabilities with Enumerate
var zipf_dist = function(n,s) {
    var pdf = mapN(function(i) { zipf_pdf(n,s,i+1) },n)
    var x = categorical({ps:pdf,vs:mapN(function(i) { i+1}, n)})
    return x
}

// Zipf PDF
var zipf_pdf = function(n,s,k) {
    return Math.pow(k,-1-s)/harmonic_number_generalized(n,s+1)
}

// Zipf CDF
var zipf_cdf = function(n,s,k) {
    return sum(mapN(function(r) {
        zipf_pdf(n,s,r+1);
    }, k))
}

// There is no closed form of the quantile so we "reverse" the CDF.
var zipf_dist_quantile = function(n,s,k) {
    var t = mapN(function(i) {
        var x = zipf_cdf(n,s,i);
        return [i,x >= k]
    },n+1);
    var tt = filter(function(x) { x[1] == true  },t)
    return first(tt)[0]

}

/*
  Zipf1
  Only parameter s, n is assmed to be Infinity, but is approximated by 1000 in zipf1_dst(s).
  If you want a better approximation, use zipf_dist(<a large n>, s) instead.
*/

var zipf1_pdf = function(s,k) {
    Math.pow(k,-1-s)/zetaf(s+1)
}

var zipf1_cdf = function(s,k) {
    return harmonic_number_generalized(Math.floor(s),1+s)/zetaf(1+s)
}

// This is just an approximation. For r < 1 it's quite unreliable
var zipf1_dist = function(s) {
    return zipf_dist(1000,s) 
}
