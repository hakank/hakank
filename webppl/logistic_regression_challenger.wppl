/*
   Logistic regression - Challenger

   See ~/jags/logistic_regression_challenger.R
   """
   model {
     for (i in 1:N){
       y[i] ~ dbern(p[i])
       p[i] <- 1 / (1 + exp(-z[i]))
       z[i] <- w0 + w1 * x[i]
     }
     w0 ~ dnorm(0, .001)
     w1 ~ dnorm(0, .0001)

   Output:
       Mean     SD  Naive SE Time-series SE
   w0 16.8900 7.7265 0.0122167       0.290180
   w1 -0.2602 0.1135 0.0001794       0.004232

   2. Quantiles for each variable:

       2.5%     25%     50%     75%    97.5%
   w0  4.131 11.3621 16.1240 21.5029 34.27954
   w1 -0.516 -0.3279 -0.2488 -0.1789 -0.07332
   """

  From https://www.zinkov.com/posts/2012-06-27-why-prob-programming-matters/
  "Logistic Regression"
  """
  Logistic Regression can be seen as a generalization of Linear Regression where the output is 
  transformed
  to lie between 0 and 1. This model only differs from the previous one by a single line, illustrating that
  adding this complexity does not require starting from scratch. The point with probabilistic programming
  is you are able to explore slightly more complex models very easily.
  """


  From https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2014-15/lecture/2015/02/27/notes-lecture3.html
  x = 66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58
  y = 1,0,1,1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,0,1,0

  The values 70 and 75 are the two valuse for which there are both true and false observations, and are
  the cause of the slightly bad prediction of about 86%.
  The p70 and p75 variables in the model are the probability that the value of 70 and 75 are false, respectively.
  The breakpoint when false switch to true is when x is around 66 (the breakpoint variable).
  
  Here's the output:

  sorted data:
  [ [ 53, false ],
    [ 57, false ],
    [ 58, false ],
    [ 63, false ],
    [ 66, true ],
    [ 67, true ],
    [ 67, true ],
    [ 67, true ],
    [ 68, true ],
    [ 69, true ],
    [ 70, false ],
    [ 70, false ],
    [ 70, true ],
    [ 70, true ],
    [ 72, true ],
    [ 73, true ],
    [ 75, false ],
    [ 75, true ],
    [ 76, true ],
    [ 76, true ],
    [ 78, true ],
    [ 79, true ],
    [ 81, true ] ]

  expectation:
  [ [ 'w0', -21.512007870050493 ],
    [ 'w1', 0.329591587231717 ],
    [ 'p70', 0.19092999999999985 ],
    [ 'p75', 0.050159999999999885 ],
    [ 'breakpoint', 64.16781000000003 ] ]

  Marginals:
  breakpoint
  Marginal:
    65 : 0.28094
    66 : 0.2748500000000003
    64 : 0.23236999999999997
    59 : 0.03869999999999999
    63 : 0.038309999999999976
    62 : 0.03651000000000002
    61 : 0.030869999999999974
    67 : 0.03052000000000001
    60 : 0.02600999999999997
    ...

  expectation:
  [ [ 'breakpoint', 64.16781000000003 ] ]


  w0: -21.512007870050493 w1: 0.329591587231717
  0 'xs[i]:' 53 'ys[i]:' false 'ypred:' 0.017231173807096992 'ybool:' false 'eq:' 'same'
  1 'xs[i]:' 57 'ys[i]:' false 'ypred:' 0.061497592254108106 'ybool:' false 'eq:' 'same'
  2 'xs[i]:' 58 'ys[i]:' false 'ypred:' 0.08350150185498557 'ybool:' false 'eq:' 'same'
  3 'xs[i]:' 63 'ys[i]:' false 'ypred:' 0.32131440631427055 'ybool:' false 'eq:' 'same'
  4 'xs[i]:' 66 'ys[i]:' true 'ypred:' 0.559969157985363 'ybool:' true 'eq:' 'same'
  5 'xs[i]:' 67 'ys[i]:' true 'ypred:' 0.6389081797034836 'ybool:' true 'eq:' 'same'
  6 'xs[i]:' 67 'ys[i]:' true 'ypred:' 0.6389081797034836 'ybool:' true 'eq:' 'same'
  7 'xs[i]:' 67 'ys[i]:' true 'ypred:' 0.6389081797034836 'ybool:' true 'eq:' 'same'
  8 'xs[i]:' 68 'ys[i]:' true 'ypred:' 0.7109947232738643 'ybool:' true 'eq:' 'same'
  9 'xs[i]:' 69 'ys[i]:' true 'ypred:' 0.7737856067282017 'ybool:' true 'eq:' 'same'
  10 'xs[i]:' 70 'ys[i]:' false 'ypred:' 0.8262677045788891 'ybool:' true 'eq:' 'not same'
  11 'xs[i]:' 70 'ys[i]:' false 'ypred:' 0.8262677045788891 'ybool:' true 'eq:' 'not same'
  12 'xs[i]:' 70 'ys[i]:' true 'ypred:' 0.8262677045788891 'ybool:' true 'eq:' 'same'
  13 'xs[i]:' 70 'ys[i]:' true 'ypred:' 0.8262677045788891 'ybool:' true 'eq:' 'same'
  14 'xs[i]:' 72 'ys[i]:' true 'ypred:' 0.9019062045003058 'ybool:' true 'eq:' 'same'
  15 'xs[i]:' 73 'ys[i]:' true 'ypred:' 0.9274510155164165 'ybool:' true 'eq:' 'same'
  16 'xs[i]:' 75 'ys[i]:' false 'ypred:' 0.9611103533650994 'ybool:' true 'eq:' 'not same'
  17 'xs[i]:' 75 'ys[i]:' true 'ypred:' 0.9611103533650994 'ybool:' true 'eq:' 'same'
  18 'xs[i]:' 76 'ys[i]:' true 'ypred:' 0.9717210965363692 'ybool:' true 'eq:' 'same'
  19 'xs[i]:' 76 'ys[i]:' true 'ypred:' 0.9717210965363692 'ybool:' true 'eq:' 'same'
  20 'xs[i]:' 78 'ys[i]:' true 'ypred:' 0.9851696183079375 'ybool:' true 'eq:' 'same'
  21 'xs[i]:' 79 'ys[i]:' true 'ypred:' 0.989289130002191 'ybool:' true 'eq:' 'same'
  22 'xs[i]:' 81 'ys[i]:' true 'ypred:' 0.9944307534768647 'ybool:' true 'eq:' 'same'

  percent correct: 0.8695652173913043

  Checking 53 .. 81
  val: 53 y: 0.017231173807096992 false
  val: 54 y: 0.023798135420963006 false
  val: 55 y: 0.032784338591305646 false
  val: 56 y: 0.0450073005399429 false
  val: 57 y: 0.061497592254108106 false
  val: 58 y: 0.08350150185498557 false
  val: 59 y: 0.11243522531729513 false
  val: 60 y: 0.14975643648431422 false
  val: 61 y: 0.1967201420212327 false
  val: 62 y: 0.2540117238379685 false
  val: 63 y: 0.32131440631427055 false
  val: 64 y: 0.3969604123020993 false
  val: 65 y: 0.4778757811704417 false
  val: 66 y: 0.559969157985363 true
  val: 67 y: 0.6389081797034836 true
  val: 68 y: 0.7109947232738643 true
  val: 69 y: 0.7737856067282017 true
  val: 70 y: 0.8262677045788891 true
  val: 71 y: 0.8686408784825517 true
  val: 72 y: 0.9019062045003058 true
  val: 73 y: 0.9274510155164165 true
  val: 74 y: 0.9467365035959 true
  val: 75 y: 0.9611103533650994 true
  val: 76 y: 0.9717210965363692 true
  val: 77 y: 0.9794985340376086 true
  val: 78 y: 0.9851696183079375 true
  val: 79 y: 0.989289130002191 true
  val: 80 y: 0.9922733183885988 true


  
  See ~/blog/logistic_regression-challenger.blog
  Compare with logistic_regression.wppl for another approach.

*/
var xs = [66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58];
var ys = [true,false,true,true,true,true,true,true,false,false,false,true,true,false,true,true,true,true,true,true,false,true,false];
// var ys = [1,0,1,1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,0,1,0];

// It's easier to see the trend and break point with sorted data
var sorted_data = sort(transpose([xs,ys]))
console.log("sorted data:")
console.log(sorted_data)

var model = function() {    
    var w0 = gaussian(0,10);
    var w1 = gaussian(0,10);

    // Restrict w0 to be negative and w1 positive.
    // This makes smaller xs values to be false and large to be true.
    condition(w0 < 0)
    condition(w1 > 0)
    
    // Here is a full version of the model
    /*
    var x = function(i) {
        // return xs[i];
        return Gaussian({mu:100,sigma:Math.sqrt(10)});
    };
    
    var z = function(i) {
        return w0 + w1 * xs[i];
    };
    
    // logistic    
    var p = function(i) {
        return 1.0/(1.0 + Math.exp(-z(i)))
    };
    
    var y = function(i) {
        return Bernoulli({p:p(i)});
    };

    // Observe x values
    mapIndexed(function(i,val) {
        observe(x(i),val);
    }, xs);
    // observe the values in y
    mapIndexed(function(i,val) {
        observe(y(i),val);
    }, ys);
    */
    
    // Putting everything in a single loop.
    /*
    mapIndexed(function(i,val) {
        var z = w0 + w1 * xs[i]
        var p = 1.0/(1.0 + Math.exp(-z))
    }, ys);
    */
    map2(function(x,y) {
        var z = w0 + w1 * x
        var p = 1.0/(1.0 + Math.exp(-z)) // logistic equation
        observe(Bernoulli({p:p}),y)        
    }, xs,ys);
    
    //
    // Find the breakpoint when false switches to true
    // Note: this use the original data xs and ys.
    var breakpoint = 1+randomInteger(_.max(xs))
    mapN(function(i) {
        factor( (xs[i] < breakpoint ? ys[i] == false : ys[i] == true) ? 0 : -20000 ) 
    },xs.length)
    
    // What is the probability that the values of 70 and 75 are false?
    var p70 = bernoulli(1.0/(1.0 + Math.exp(-(w0 + w1 * 70)))) == false
    var p75 = bernoulli(1.0/(1.0 + Math.exp(-(w0 + w1 * 75)))) == false    
    
    return {
        w0:w0,
        w1:w1,
        p70:p70,        
        p75:p75,
        breakpoint:breakpoint,
    };

}

// var d = Infer(model);
// var d = Infer({method:"MCMC",samples:10000,burn:1000,verbose:true},model);
var d = Infer({method:"MCMC",kernel:"MH",samples:100000,verbose:false},model);
// var d = Infer({method:"SMC",particles:10000},model);
// var d = Infer({method:"asyncPF",samples:10000,burn:2000},model);

console.log()
exp_map_all(d,["expectation"]);
exp_map(d,["breakpoint"],["marginals","expectation"]);

/*
showCredibleInterval(d,"w0",0.93)
showCredibleInterval(d,"w1",0.93)
*/

// v0 vs w1 which clearly shows the shift in in w1 when w0 changes from 0 to 3
// Note: Use --require webppl-viz  for this.
// viz(d)

/*
  Post check and analysis
*/

var w0 = expectation(marginalize(d, "w0"))
var w1 = expectation(marginalize(d, "w1"))
console.log("\nw0:",w0,"w1:",w1)

var [xs2,ys2] = transpose(sorted_data)
var t = sum(mapN(function(i) {
    var y = 1.0/(1.0 + Math.exp(- ( w0 + w1*xs2[i]  ) ))
    var ybool = Math.round(y) == 1 ? true : false
    console.log(i,"xs[i]:",xs2[i],"ys[i]:",ys2[i], "ypred:",y,"ybool:",ybool, "eq:", ys2[i] == ybool ? "same" : "not same")
    return ys2[i] == ybool
},xs2.length))
console.log("\npercent correct:", t/xs2.length)


var min_val = _.min(xs2)
var max_val = _.max(xs2)
console.log("\nChecking",min_val,"..",max_val)
var t = mapN(function(i) {
    var val = i+min_val
    var y = 1.0/(1.0 + Math.exp(- ( w0 + w1*val  ) ))
    console.log("val:",val,"y:",y, Math.round(y) == 1)
}, max_val-min_val)

