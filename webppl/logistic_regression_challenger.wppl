/*

   Logistic regression - Challenger

   See ~/jags/logistic_regression_challenger.R
   """
   model {
     for (i in 1:N){
       y[i] ~ dbern(p[i])
       p[i] <- 1 / (1 + exp(-z[i]))
       z[i] <- w0 + w1 * x[i]
     }
     w0 ~ dnorm(0, .001)
     w1 ~ dnorm(0, .0001)

   Output:
      Mean     SD  Naive SE Time-series SE
w0 16.8900 7.7265 0.0122167       0.290180
w1 -0.2602 0.1135 0.0001794       0.004232

2. Quantiles for each variable:

     2.5%     25%     50%     75%    97.5%
w0  4.131 11.3621 16.1240 21.5029 34.27954
w1 -0.516 -0.3279 -0.2488 -0.1789 -0.07332
   """

  From https://www.zinkov.com/posts/2012-06-27-why-prob-programming-matters/
  "Logistic Regression"
  """
  Logistic Regression can be seen as a generalization of Linear Regression where the output is 
  transformed
  to lie between 0 and 1. This model only differs from the previous one by a single line, illustrating that
  adding this complexity does not require starting from scratch. The point with probabilistic programming
  is you are able to explore slightly more complex models very easily.
  """


  From https://www.stat.ubc.ca/~bouchard/courses/stat520-sp2014-15/lecture/2015/02/27/notes-lecture3.html
  x = 66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58
  y = 1,0,1,1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,0,1,0

  expectation:
  [ [ 'w0', -19.83132118958726 ], [ 'w1', 0.3074071932803347 ] ]

  MAP:
  [ [ 'w0',
    { val: -20.050566447078932, score: -2.2566561611056453 } ],
    [ 'w1', { val: 0.31319026843157, score: -1.320881222571658 } ] ]

  Credible interval for w0 (93%): [ -21.12140766619648, -17.751603081708623 ]
  Credible interval for w1 (93%): [ 0.2839382608999339, 0.3210839623342104 ]

  
  See ~/blog/logistic_regression-challenger.blog
  Compare with logistic_regression.wppl for another approach.

*/

var model = function() {
    var xs = [66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58];
    var ys = [true,false,true,true,true,true,true,true,false,false,false,true,true,false,true,true,true,true,true,true,false,true,false];
    // var ys = [1,0,1,1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,0,1,0];

    
    var w0 = gaussian(0,Math.sqrt(100));
    var w1 = gaussian(0,Math.sqrt(100));
    // var w0 = gaussian(0,Math.sqrt(1/.001));
    // var w1 = gaussian(0,Math.sqrt(1/.0001));
    
    var x = function(i) {
        // return xs[i];
        return Gaussian({mu:100,sigma:Math.sqrt(10)});
    };
    
    var z = function(i) {
        return w0 + w1 * xs[i];
    };
    
    // logistic    
    var p = function(i) {
        return 1.0/(1.0 + Math.exp(-z(i)));
    };
    
    var y = function(i) {
        return Bernoulli({p:p(i)});
    };

    // Observe x values
    mapIndexed(function(i,val) {
        observe(x(i),val);
    }, xs);
    
    // observe the values in y
    mapIndexed(function(i,val) {
        observe(y(i),val);
        // observe(Bernoulli({p:1.0/(1.0 + Math.exp(-z(i)))}),val)
        // observe(Bernoulli({p:1.0/(1.0 + Math.exp(-(w0 + w1 * xs[i])))}),val)
    }, ys);


    
    return {w0:w0,
            w1:w1
           };

}

// var d = Infer(model);
// var d = Infer({method:"MCMC",samples:10000,burn:1000,verbose:true},model);
var d = Infer({method:"MCMC",samples:10000,verbose:true},model);
// var d = Infer({method:"SMC",particles:10000},model);
// var d = Infer({method:"asyncPF",samples:10000,burn:2000},model);
// display(d);

// display(d["w0"])

exp_map(d,["w0","w1"],["expectation","MAP"]);

showCredibleInterval(d,"w0",0.93)
showCredibleInterval(d,"w1",0.93)

// v0 vs w1 which clearly shows the shift in in w1 when w0 changes from 0 to 3
// viz(d)
