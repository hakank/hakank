/*
  Choosing a restaurant

  From 
  A Bayesian Way of Choosing a Restaurant
  https://towardsdatascience.com/a-bayesian-way-of-choosing-a-restaurant-87905a745854
  """
  Recently I was looking for a new good restaurant. Google Maps highlighted me 2 options: 
  restaurant A with 10 reviews all 5 stars and restaurant B with 200 reviews and average 
  rating 4. I tempted to choose restaurant A but the low number of reviews concerned me. 
  On the other hand, many reviews of restaurant B gave me confidence of its 4 star rating, 
  but promised nothing excellent. So, I wanted to compare the restaurants and choose the 
  best discounting on reviews or lack of reviews. Thanks to Bayes, there is a way.

  ...

  To update the initial beliefs we need to multiply the prior beliefs to the likelihood of 
  observing the data with the prior beliefs.
  The observed data is naturally described by Multinomial distribution (generalization of 
  Binomial).
  It turns out that Dirichlet is a conjugate prior to the Multinomial likelihood. In other 
  words our posterior distr. is also a Dirichlet distributuion with parameters incorporating 
  observed data.

  ...
  The posterior avg. rating of A is now somewhere in the middle between prior 3 and 
  observed 5. But the avg. rating of B didnâ€™t change much because the large number 
  of reviews outweighted the initial beliefs.

  ...
  In my case I obtain the probability of 85% that restaurant A is better than restaurant B.


  --- 
  https://github.com/uselessskills/back-of-the-envelope/blob/eb487cfd1a577518e24af478081af68f95c6db30/rating_uncertainty.ipynb

  """

  The original was using a Dirichlet for the likelihood but that didn't show any 
  difference between A and B.

  However, using Multinomial() is much better: the difference is about 89%.

  p
  Marginal:
    true : 0.8848999999999992
    false : 0.11510000000000079

  expectation:
  [ [ 'prior_reviews_mean_a', 2.9923060018723926 ],
    [ 'prior_reviews_mean_b', 2.973150088671328 ],
    [ 'post_reviews_mean_a', 4.297282618826059 ],
    [ 'post_reviews_mean_b', 3.9156989701985427 ],
    [ 'diff', 0.38158364862752037 ],
    [ 'p_a0', 0.07638933034640556 ],
    [ 'p_a1', 0.06279169890955494 ],
    [ 'p_a2', 0.07396734267101084 ],
    [ 'p_a3', 0.06085027771762695 ],
    [ 'p_a4', 0.7260013503554014 ],
    [ 'p_b0', 0.12636858630227998 ],
    [ 'p_b1', 0.0311721967013776 ],
    [ 'p_b2', 0.05483554502708689 ],
    [ 'p_b3', 0.37563900443403087 ],
    [ 'p_b4', 0.4119846675352248 ],
    [ 'p', 0.8848999999999992 ] ]

  var diff
  num_undefined: 0
  Num samples: 20000
  min: -1.269792052936717
  listMean: 0.3815836486275238
  listVar: 0.09813798665290284
  listStdev: 0.31326983042243767
  max: 0.8539749291965997

  Percentiles:
  [ [ 0, -1.269792052936717 ],
    [ 1, -0.682940116552579 ],
    [ 2.5, -0.36198170481875014 ],
    [ 25, 0.2695334943195471 ],
    [ 50, 0.365049581697761 ],
    [ 75, 0.609164986295772 ],
    [ 97.5, 0.8332555192536164 ],
    [ 99, 0.8332555192536164 ],
    [ 100, 0.8539749291965997 ] ]

  Histogram ( 20000 samples)
  Bin interval | #Obs | Rel. freq. (prob / cumulative prob)
 -1.27    3  ( 0 / 0 )
 -1.23    0  ( 0 / 0 )
 -1.19    0  ( 0 / 0 )
 -1.16    5  ( 0 / 0 )
 -1.12    0  ( 0 / 0 )
 -1.08    0  ( 0 / 0 )
 -1.04    0  ( 0 / 0 )
    -1    0  ( 0 / 0 )
 -0.97    0  ( 0 / 0 )
 -0.93   12  ( 0.001 / 0.001 )
 -0.89    0  ( 0 / 0.001 )
 -0.85    0  ( 0 / 0.001 )
 -0.81    0  ( 0 / 0.001 )
 -0.78   38 * ( 0.002 / 0.003 )
 -0.74    0  ( 0 / 0.003 )
  -0.7  137 ***** ( 0.007 / 0.01 )
 -0.66   38 * ( 0.002 / 0.012 )
 -0.63    0  ( 0 / 0.012 )
 -0.59    0  ( 0 / 0.012 )
 -0.55    4  ( 0 / 0.012 )
 -0.51   23 * ( 0.001 / 0.013 )
 -0.47    0  ( 0 / 0.013 )
 -0.44   59 ** ( 0.003 / 0.016 )
  -0.4   57 ** ( 0.003 / 0.019 )
 -0.36  206 ******** ( 0.01 / 0.029 )
 -0.32   72 *** ( 0.004 / 0.033 )
 -0.28    3  ( 0 / 0.033 )
 -0.25  120 ***** ( 0.006 / 0.039 )
 -0.21   78 *** ( 0.004 / 0.043 )
 -0.17   72 *** ( 0.004 / 0.046 )
 -0.13  164 ****** ( 0.008 / 0.055 )
 -0.09  602 ************************ ( 0.03 / 0.085 )
 -0.06  510 ******************** ( 0.026 / 0.11 )
 -0.02    0  ( 0 / 0.11 )
  0.02  474 ******************* ( 0.024 / 0.134 )
  0.06   76 *** ( 0.004 / 0.138 )
   0.1  364 ************** ( 0.018 / 0.156 )
  0.13  332 ************* ( 0.017 / 0.172 )
  0.17  899 *********************************** ( 0.045 / 0.217 )
  0.21  440 ***************** ( 0.022 / 0.239 )
  0.25  169 ******* ( 0.008 / 0.248 )
  0.29  948 ************************************* ( 0.047 / 0.295 )
  0.32 1856 ************************************************************************* ( 0.093 / 0.388 )
  0.36 1936 **************************************************************************** ( 0.097 / 0.485 )
   0.4 1394 ******************************************************* ( 0.07 / 0.555 )
  0.44  170 ******* ( 0.009 / 0.563 )
  0.47  557 ********************** ( 0.028 / 0.591 )
  0.51  435 ***************** ( 0.022 / 0.613 )
  0.55 2033 ******************************************************************************** ( 0.102 / 0.714 )
  0.59  494 ******************* ( 0.025 / 0.739 )
  0.63  913 ************************************ ( 0.046 / 0.785 )
  0.66    0  ( 0 / 0.785 )
   0.7  374 *************** ( 0.019 / 0.803 )
  0.74  923 ************************************ ( 0.046 / 0.85 )
  0.78 1573 ************************************************************** ( 0.079 / 0.928 )
  0.82    0  ( 0 / 0.928 )
 >0.82 1437 ********************************************************* ( 0.072 / 1 )



*/

// Add 1 to each value to avoid 0s
var add1 = function(a) {
    return map(function(v) { v+1}, a)
}

// The dot function for arrays.
// T.dot requires matrices and cannot handle arrays or vectors
var dot = function(x,y) {
    sum(mapN(function(i) { x[i]*y[i] },x.length)) 
}

// Scale all values in a by scale scale
var scale = function(a,scale) {
    map(function(v) { v / scale },a)
}

// observed data
var reviews_a = [0, 0, 0, 0, 10]
var reviews_a_add1 = add1(reviews_a)
var reviews_a_v = Vector(reviews_a)
var reviews_a_sum = sum(reviews_a)
var reviews_a_s = scale(add1(reviews_a),sum(add1(reviews_a))) // add1 and scale 

var reviews_b = [21, 5, 10, 79, 85]
var reviews_b_add1 = add1(reviews_b)
var reviews_b_v = Vector(reviews_b)
var reviews_b_sum = sum(reviews_b)
var reviews_b_s = scale(add1(reviews_b),sum(add1(reviews_b))) // add1 and scale

var n = reviews_b.length

console.log("reviews_a_s:",reviews_a_s)
console.log("reviews_b_s:",reviews_b_s)


var ratings_support = [1, 2, 3, 4, 5]
var ratings_support_v = Vector(ratings_support)

console.log("observed mean A :", dot(reviews_a,ratings_support)/sum(reviews_a))
console.log("observed mean B :", dot(reviews_b,ratings_support)/sum(reviews_b))

console.log("ratings mean (prior):", dot([1,1,1,1,1],ratings_support)/sum([1,1,1,1,1]))

var model = function() {
    // Priors
    var p_a = dirichlet(ones([n,1]))
    var p_a_s = T.toScalars(p_a) // Scalar version for dot multiplications
    
    var p_b = dirichlet(ones([n,1]))
    var p_b_s = T.toScalars(p_b)

    // Priors for prior reviews_mean_*
    var p_a_prior = T.toScalars(dirichlet(ones([n,1])))
    var p_b_prior = T.toScalars(dirichlet(ones([n,1])))    
    
    // T.dot requires two matrices of the same rank. I.e. it does not supports vectors.
    // prior_reviews_mean_a = np.dot(p_a_prior, ratings_support)
    // prior_reviews_mean_b = np.dot(p_b_prior, ratings_support)
    var prior_reviews_mean_a = sum(mapN(function(i) { p_a_prior[i]*ratings_support[i] },n))
    var prior_reviews_mean_b = sum(mapN(function(i) { p_b_prior[i]*ratings_support[i] },n))


    // Using Multinomial for observing the data
    observe(Multinomial({ps:p_a_s,n:sum(reviews_a)}),reviews_a)
    observe(Multinomial({ps:p_b_s,n:sum(reviews_b)}),reviews_b)
    
    var post_reviews_mean_a = dot(p_a_s,ratings_support)
    var post_reviews_mean_b = dot(p_b_s,ratings_support)

    var diff = post_reviews_mean_a - post_reviews_mean_b
    
    // Is A better than B?
    var p = post_reviews_mean_a > post_reviews_mean_b
    
    return {
        prior_reviews_mean_a:prior_reviews_mean_a,
        prior_reviews_mean_b:prior_reviews_mean_b,
        
        post_reviews_mean_a:post_reviews_mean_a,
        post_reviews_mean_b:post_reviews_mean_b,
        diff:diff,
        // p_a:p_a,
        p_a0:p_a_s[0],
        p_a1:p_a_s[1],
        p_a2:p_a_s[2],
        p_a3:p_a_s[3],
        p_a4:p_a_s[4],        
        // p_b:p_b,
        p_b0:p_b_s[0],
        p_b1:p_b_s[1],
        p_b2:p_b_s[2],
        p_b3:p_b_s[3],
        p_b4:p_b_s[4],        
        p:p,
    }
}

// var d = Infer(model)
// var d = Infer({method:"rejection",samples:1000},model)
// var d = Infer({method:"SMC",particles:10000,rejuvSteps:0},model)
var d = Infer({method:"MCMC",kernel:"MH",samples:10000},model)
exp_map_all(d,["expectation","marginals"])

show_stats(d,"post_reviews_mean_a")
show_stats(d,"post_reviews_mean_b")
show_stats(d,"diff")
